{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "This project folder contains all the moduls and scripts necessary to investigate the question, whether articles nominated for featured article status differ in the ammount of eddits which are conducted during the nomination periode and shortly before, depending on whether the nomination be sucessfull or not. \n",
    "\n",
    "**The main documention is included in the respective scripts / moduls!**\n",
    "\n",
    "## Navigating the project\n",
    "run_skript.ipynb is the intented entry point of the project. From here you can step through the initial data generation process. The main functionality is implemented in 2 sax parsers. \n",
    "* parse_nominations.py\n",
    "* parse_ends.py\n",
    "\n",
    "This results in 2 .csv files and 1 dictionary. \n",
    "* fac_nomination.csv\n",
    "* fa_nomination.csv\n",
    "* articles_dict.pkl \n",
    "\n",
    "The .csv-files contain all nominations, the time of their nomination, the time of the last comment. articles_dict the timestamp of each revision of the featured article discussion, in which an article came up. \n",
    "In the merge.ipynb script this information is used determine the nomination period of all articles. Here we also handle some coner cases, perform sanity checks and have a first look at the data. \n",
    "\n",
    "\n",
    "revisions.py defines a simple dataclass to consitently store the result of parsing\n",
    "\n",
    "There are 2 additional files\n",
    "* regex_dict.ipynb\n",
    "* wiki_api.py\n",
    "\n",
    "The first one explaines the rather (uneseccarily) complex 2 liner to extract the dates in parse_nominations.py. I came back to this project with with a little more experience as when I started it, and after I reduced the original java code by nearly 50 lines, I wanted to opimize this little piece of code and wrote down my train of thought. It is unnescary complex because a) writing it a bit more verbose makes it fairly clear what happens there and b) later we import the into a pandas data fram which has utilities to automatically detect date formats from strings. \n",
    "\n",
    "wiki_api.py contains several 2 alternatives to the wiki dumps for getting revision information on articles. The first one directly contacts the wiki api while the second one uses the very comfortable pywikibot library for the task. \n",
    "\n",
    "\n",
    "* parse_revision.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
