{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "This project folder contains all the modules and scripts necessary to investigate the question, whether articles nominated for featured article status differ in the number of edits and authors during the nomination period and shortly before, depending on whether the nomination be successful or not. \n",
    "\n",
    "**The main documentation is included in the respective scripts / modules!**\n",
    "\n",
    "## Navigating the project\n",
    "run_skript.ipynb is the intended entry point of the project. From here you can step through the initial data generation process. The main functionality is implemented in 2 sax parsers. \n",
    "* parse_nominations.py\n",
    "* parse_ends.py\n",
    "\n",
    "This results in 2 .csv files and 1 dictionary. \n",
    "* fac_nomination.csv /fa_nomination.csv\n",
    "* articles_dict.pkl \n",
    "\n",
    "The .csv-files contain all nominations, the time of their nomination, the time of the last comment. articles_dict contains the timestamp of each revision of the featured article discussion, in which an article was present. \n",
    "In the next step we combine both combine these to determine the nomination period. The corresponding functions are defined in \n",
    "* process.py\n",
    "\n",
    "and result merged contain all the information,and control is mininal version for processing the revisions.\n",
    "* fac_merged.csv / fa_merged.csv\n",
    "* fac_controll.csv / fa_controll.csv\n",
    "\n",
    "Finally we parse find use a 3rd SAX parser to process stubs.meta.history to find the number of edits and authors. Previously we recorded more detailed information on the articles. To speed up the processing of the data we have removed all unnecessary functionality.\n",
    "* parse_revision.py\n",
    "\n",
    "input:\n",
    "data/enwiki_history.xml.gz\n",
    "res/fac_controll.csv / fa_controll.csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "There are 3 additional files\n",
    "* regex_dict.ipynb\n",
    "* wiki_api.py\n",
    "* revisions.py\n",
    "\n",
    "The first one explains the rather (unnecessarily) complex 2 liner to extract the dates in parse_nominations.py. I came back to this project with with a little more experience as when I started it, and after I reduced the original java code by nearly 50 lines, I wanted to opimize this little piece of code and wrote down my train of thought. It is unnescary complex because a) writing it a bit more verbose makes it fairly clear what happens there and b) later we import the dates into a pandas data frame which has utilities to automatically detect date formats from strings. \n",
    "\n",
    "wiki_api.py contains several 2 alternatives to the wiki dumps for getting revision information on articles. The first one directly contacts the wiki api while the second one uses the very comfortable pywikibot library for the task. \n",
    "\n",
    "revisions.py contains dataclasses to consitently store information, once for pywikibot and once for xml parsing. Since we only recorded edtis and unique authors they were not used in the final version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
