{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/max/pyscripts/wiki_project_local/wiki_project\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import os \n",
    "import sys\n",
    "from importlib import reload as irl\n",
    "from parse_nominations import MyNominationHandler\n",
    "from parse_ends import MyEndHandler\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "# some Ipython 'magic' to reload moduls\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get links to Featured Article Archives\n",
    "Here we take the html file of the Featured Article Archive page and extract the links to the monthly archives. Subsequently we supply them to wiki.Specail:Export and extract the xml file. It's important  to export templates as well, otherwise the discussions are obmitted.  \n",
    "* (https://en.wikipedia.org/wiki/Wikipedia:Featured_article_candidates/Featured_log)\n",
    "* (https://en.wikipedia.org/wiki/Wikipedia:Featured_article_candidates/Archived_nominations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_archives(filename, pattern):\n",
    "    \"\"\" Retrives the links to the monthly FA-Archives\"\"\"\n",
    "    \n",
    "    # Extract links from Html document\n",
    "    with open(filename) as file:\n",
    "        links_to_archive = []\n",
    "        for line in file:\n",
    "            matcher = re.search(pattern, line)\n",
    "            if matcher:\n",
    "                links_to_archive.append(matcher.group(1))\n",
    "                    \n",
    "    \n",
    "    # Remove links to archives from before 2005 and after 2016\n",
    "    year_list = ['2003', '2004', '2017', '2018', '2019'] \n",
    "    res = []\n",
    "    for link in links_to_archive:\n",
    "        if not any([year in link for year in year_list]):\n",
    "            res.append(link)\n",
    "    return res\n",
    "\n",
    "# sucessful nominations\n",
    "pattern_featured = \"(Wikipedia:Featured_article_candidates/Featured_log/.*?)\\\"\"\n",
    "with open('./data/FA_archives.txt', 'w') as f:\n",
    "    for archive in get_archives('data/FAArchive.html', pattern_featured):\n",
    "        f.write(archive +'\\n')\n",
    "        \n",
    "#unsucessful nominations\n",
    "pattern_candidate = \"(Wikipedia:Featured_article_candidates/Archived_nominations/.*?)\\\"\"\n",
    "with open('./data/FAC_archives.txt', 'w') as f:\n",
    "    for archive in get_archives('data/FACArchive.html', pattern_candidate)[1:]:\n",
    "        f.write(archive +'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wirte the names of all articles to a file for checking later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/FAC_all_archives.xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-92125c2d83a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mFAC_articles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./log/FAC_all_articles.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#get_articles(FA_archives, FA_articles)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mget_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFAC_archives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFAC_articles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-92125c2d83a6>\u001b[0m in \u001b[0;36mget_articles\u001b[0;34m(in_file, out_file)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0marticles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/FAC_all_archives.xml'"
     ]
    }
   ],
   "source": [
    "FA_archives = './data/FA_all_archives.xml'\n",
    "FA_articles = './data/FA_all_articles.txt'\n",
    "\n",
    "def get_articles(in_file, out_file):\n",
    "    with open(in_file, 'r') as f:\n",
    "        articles = []\n",
    "        for line in f:\n",
    "            match =re.search(\"Wikipedia:Featured article candidates/(.*)/archive[\\\\d]\", line)\n",
    "            if match: #None is falsey in Python\n",
    "                articles.append(match.group(1)) \n",
    "    with open(out_file, 'w') as file:\n",
    "        for art in articles:\n",
    "            file.write(art +'\\n')\n",
    "\n",
    "            \n",
    "FAC_archives = './data/FAC_all_archives.xml'\n",
    "FAC_articles = './log/FAC_all_articles.txt'\n",
    "#get_articles(FA_archives, FA_articles)\n",
    "get_articles(FAC_archives, FAC_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the XML files\n",
    "The next step is to go to (wiki/Special:Export)[https://en.wikipedia.org/wiki/Special:Export] and use the content of FA(C)_archives to export the current state, including templates of alle Featured Articles. The two resulting files were named FA.xml (for the sucessfull) and FAC.xml (for the unsucessfull ones). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the nominated articles and time of nomination\n",
    "MyNominationHandler is in charge of \n",
    "1. finding all candidates\n",
    "2. find the point in time when they were nominated\n",
    "3. find the last comment (will be used in handling double nominations)\n",
    "\n",
    "It will produce an .csv file containing all nominations found in the .xml file and write these to ./data/ . A dictionary containing all problematic entries will be stored in ./tmp/ as a pickle object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0 is out of bounds for axis 0 with size 0\n",
      "Problematic entry in 5114\n",
      "seems like no title was found.\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "Problematic entry in 5291\n",
      "seems like no title was found.\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "Problematic entry in 6087\n",
      "seems like no title was found.\n"
     ]
    }
   ],
   "source": [
    "#Parse Sucessful Nominations\n",
    "nom_handler = MyNominationHandler('./data/FA_nomination.csv', './tmp/FA_prob.pkl')\n",
    "nom_handler.parse('./data/FA.xml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem parsing date in 613\n"
     ]
    }
   ],
   "source": [
    "# Parse Unsucsessful Nominations\n",
    "nom_handler = MyNominationHandler('./data/FAC_nomination.csv', './tmp/FAC_prob.pkl')\n",
    "nom_handler.parse('./data/FAC.xml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ends_handler = MyEndHandler('./res/FACends.txt')\n",
    "nom_handler.parse('./data/FAChistory.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all all revisions in which an article was present\n",
    "To determine the end of the nomination period the last comment is an unreliable indicator. \n",
    "Thus we parse the whole revision history of the featured article discussion and store all articles currently nominated. \n",
    "Since we need more than the last 1000 revisions, we cannot use wiki/Special:Export. I used [WikiEvent](http://algo.uni-konstanz.de/software/wikievent/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2004-02-13 19:07:35 is already in the dict. \n",
      " This schould not have happend! (More than once)\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "end_handler = MyEndHandler('data/FAC_ends.pkl', 'tmp/FAC_ends_prob.pkl')\n",
    "end_handler.parse('./data/FAChistory.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Wiki Project",
   "language": "python",
   "name": "wiki_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
