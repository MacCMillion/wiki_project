{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/max/pyscripts/wiki_project_local/wiki_project\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import os \n",
    "import sys\n",
    "import gzip\n",
    "\n",
    "\n",
    "from parse_nominations import MyNominationHandler\n",
    "from parse_ends import MyEndHandler\n",
    "from parse_revision import MyRevisionsHandler\n",
    "from parse_revision_2wlater import MyRevisionsHandler as MyRevisionsHandler_2w\n",
    "import process\n",
    "import config\n",
    "\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "# some Ipython 'magic' to reload moduls\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get links to Featured Article Archives\n",
    "Here we take the html file of the Featured Article Archive page and extract the links to the monthly archives. Subsequently we supply them to wiki.Specail:Export and extract the xml file. It's important  to export templates as well, otherwise the discussions are obmitted.  \n",
    "* (https://en.wikipedia.org/wiki/Wikipedia:Featured_article_candidates/Featured_log)\n",
    "* (https://en.wikipedia.org/wiki/Wikipedia:Featured_article_candidates/Archived_nominations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "# sucessful nominations\n",
    "pattern_featured = \"(Wikipedia:Featured_article_candidates/Featured_log/.*?)\\\"\"\n",
    "with open('./data/FA_archives.txt', 'w') as f:\n",
    "    for archive in process.get_archives('data/FAArchive.html', pattern_featured):\n",
    "        f.write(archive +'\\n')\n",
    "        \n",
    "#unsucessful nominations\n",
    "pattern_candidate = \"(Wikipedia:Featured_article_candidates/Archived_nominations/.*?)\\\"\"\n",
    "with open('./data/FAC_archives.txt', 'w') as f:\n",
    "    for archive in process.get_archives('data/FACArchive.html', pattern_candidate)[1:]:\n",
    "        f.write(archive +'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the XML files\n",
    "The next step is to go to [wiki/Special:Export](https://en.wikipedia.org/wiki/Special:Export) and use the content of FA(C)_archives to export the current state, including templates of alle Featured Articles. The two resulting files were named FA.xml (for the sucessfull) and FAC.xml (for the unsucessfull ones). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the nominated articles and time of nomination\n",
    "MyNominationHandler is in charge of \n",
    "1. finding all candidates\n",
    "2. find the point in time when they were nominated\n",
    "3. find the last comment (will be used in handling double nominations)\n",
    "\n",
    "It will produce an .csv file containing all nominations found in the .xml file and write these to ./data/ . A dictionary containing all problematic entries will be stored in ./tmp/ as a pickle object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0 is out of bounds for axis 0 with size 0\n",
      "Problematic entry in 5114\n",
      "seems like no title was found.\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "Problematic entry in 5291\n",
      "seems like no title was found.\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "Problematic entry in 6087\n",
      "seems like no title was found.\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "#Parse Sucessful Nominations\n",
    "nom_handler = MyNominationHandler('./data/FA_nomination.csv', './tmp/FA_prob.pkl')\n",
    "nom_handler.parse('./data/FA.xml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem parsing date in 613\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "# Parse Unsucsessful Nominations\n",
    "nom_handler = MyNominationHandler('./data/F:AC_nomination.csv', './tmp/FAC_prob.pkl')\n",
    "nom_handler.parse('./data/FAC.xml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all all revisions in which an article was present\n",
    "To determine the end of the nomination period the last comment is an unreliable indicator. \n",
    "Thus we parse the whole revision history of the featured article discussion and store all articles currently nominated. \n",
    "Since we need more than the last 1000 revisions, we cannot use wiki/Special:Export. I used [WikiEvent](http://algo.uni-konstanz.de/software/wikievent/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2004-02-13 19:07:35 is already in the dict. \n",
      " This schould not have happend! (More than once)\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "end_handler = MyEndHandler('data/FAC_ends.pkl', 'tmp/FAC_ends_prob.pkl')\n",
    "end_handler.parse('./data/FAChistory.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and merge data\n",
    "#### Sucessfull nominations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiple nominations: 50\n",
      "number of nominations:                 5280 \n",
      "artilcles in ends:                10451\n",
      "unique articles in nominations:       5255 \n",
      "articles in merged data Frame:            5157\n",
      "5157 10451\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "FA_controll = process.merge('./data/FA_nomination.csv', './data/FAC_ends.pkl')\n",
    "FA_controll = process.decide_end(FA_controll)\n",
    "FA_controll.drop('dates', 1)\n",
    "FA_controll.to_csv('./res/FA_merged.csv', sep=';')\n",
    "FA_controll = FA_controll[['title', 'start_date', 'date_nomination', 'end_date']]\n",
    "FA_controll.to_csv('./data/FA_controll.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsucessfull nominations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiple nominations: 1845\n",
      "number of nominations:                 4966 \n",
      "artilcles in ends:                10451\n",
      "unique articles in nominations:       3885 \n",
      "articles in merged data Frame:            3774\n",
      "3774 10451\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "FAC_controll = process.merge('./data/FAC_nomination.csv', './data/FAC_ends.pkl')\n",
    "FAC_controll = process.decide_end(FAC_controll)\n",
    "FAC_controll.drop('dates', 1)\n",
    "FAC_controll.to_csv('./res/FAC_merged.csv', sep=';')\n",
    "FAC_controll = FAC_controll[['title', 'start_date', 'date_nomination', 'end_date']]\n",
    "FAC_controll.to_csv('./data/FAC_controll.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "with gzip.open('./data/enwiki_history.xml.gz') as input_file:\n",
    "    MyRevisionsHandler('./res/revisions_FAC.csv', './data/FAC_controll.csv', True).parse(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%autoreload` not found.\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "with gzip.open('./data/enwiki_history.xml.gz') as input_file:\n",
    "    MyRevisionsHandler('./res/revisions_FA.csv', './data/FA_controll.csv', True).parse(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FA_controll['date_2w_later'] = FA_controll['end_date'] + np.timedelta64(2, 'W')\n",
    "FA_controll = FA_controll.loc[:, ['title', 'end_date', 'date_2w_later']]\n",
    "FA_controll.to_csv('./data/FA_controll_2w.csv', sep=';')\n",
    "\n",
    "FAC_controll['date_2w_later'] = FAC_controll['end_date'] + np.timedelta64(2, 'W')\n",
    "FAC_controll = FAC_controll.loc[:, ['title', 'end_date', 'date_2w_later']]\n",
    "FAC_controll.to_csv('./data/FA_controll_2w.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('./data/enwiki_history.xml.gz') as input_file:\n",
    "    MyRevisionsHandler_2w('./res/revisions_FA_2w.csv', './data/FA_controll_2w.csv', True).parse(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-05 03:06:48.796881\n",
      "2019-04-05 13:41:11.900210\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "print(datetime.now())\n",
    "with gzip.open('./data/enwiki_history.xml.gz') as input_file:\n",
    "    MyRevisionsHandler_2w('./res/revisions_FAC_2w.csv', './data/FAC_controll_2w.csv', True).parse(input_file)\n",
    "print(datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Wiki Project",
   "language": "python",
   "name": "wiki_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
