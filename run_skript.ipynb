{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/max/pyscripts/wiki_project_local/wiki_project\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import os \n",
    "import sys\n",
    "import gzip\n",
    "\n",
    "\n",
    "from parse_nominations import MyNominationHandler\n",
    "from parse_ends import MyEndHandler\n",
    "from parse_revision import MyRevisionsHandler\n",
    "import process\n",
    "import config\n",
    "\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "# some Ipython 'magic' to reload moduls\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get links to Featured Article Archives\n",
    "Here we take the html file of the Featured Article Archive page and extract the links to the monthly archives. Subsequently we supply them to wiki.Specail:Export and extract the xml file. It's important  to export templates as well, otherwise the discussions are obmitted.  \n",
    "* (https://en.wikipedia.org/wiki/Wikipedia:Featured_article_candidates/Featured_log)\n",
    "* (https://en.wikipedia.org/wiki/Wikipedia:Featured_article_candidates/Archived_nominations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "def get_archives(filename, pattern):\n",
    "    \"\"\" Retrives the links to the monthly FA-Archives\"\"\"\n",
    "    \n",
    "    # Extract links from Html document\n",
    "    with open(filename) as file:\n",
    "        links_to_archive = []\n",
    "        for line in file:\n",
    "            matcher = re.search(pattern, line)\n",
    "            if matcher:\n",
    "                links_to_archive.append(matcher.group(1))\n",
    "                    \n",
    "    \n",
    "    # Remove links to archives from before 2005 and after 2016\n",
    "    year_list = ['2003', '2004', '2017', '2018', '2019'] \n",
    "    res = []\n",
    "    for link in links_to_archive:\n",
    "        if not any([year in link for year in year_list]):\n",
    "            res.append(link)\n",
    "    return res\n",
    "\n",
    "# sucessful nominations\n",
    "pattern_featured = \"(Wikipedia:Featured_article_candidates/Featured_log/.*?)\\\"\"\n",
    "with open('./data/FA_archives.txt', 'w') as f:\n",
    "    for archive in get_archives('data/FAArchive.html', pattern_featured):\n",
    "        f.write(archive +'\\n')\n",
    "        \n",
    "#unsucessful nominations\n",
    "pattern_candidate = \"(Wikipedia:Featured_article_candidates/Archived_nominations/.*?)\\\"\"\n",
    "with open('./data/FAC_archives.txt', 'w') as f:\n",
    "    for archive in get_archives('data/FACArchive.html', pattern_candidate)[1:]:\n",
    "        f.write(archive +'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wirte the names of all articles to a file for checking later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/FAC_all_archives.xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-92125c2d83a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mFAC_articles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./log/FAC_all_articles.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#get_articles(FA_archives, FA_articles)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mget_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFAC_archives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFAC_articles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-92125c2d83a6>\u001b[0m in \u001b[0;36mget_articles\u001b[0;34m(in_file, out_file)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0marticles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/FAC_all_archives.xml'"
     ]
    }
   ],
   "source": [
    "FA_archives = './data/FA_all_archives.xml'\n",
    "FA_articles = './data/FA_all_articles.txt'\n",
    "\n",
    "def get_articles(in_file, out_file):\n",
    "    with open(in_file, 'r') as f:\n",
    "        articles = []\n",
    "        for line in f:\n",
    "            match =re.search(\"Wikipedia:Featured article candidates/(.*)/archive[\\\\d]\", line)\n",
    "            if match: #None is falsey in Python\n",
    "                articles.append(match.group(1)) \n",
    "    with open(out_file, 'w') as file:\n",
    "        for art in articles:\n",
    "            file.write(art +'\\n')\n",
    "\n",
    "            \n",
    "FAC_archives = './data/FAC_all_archives.xml'\n",
    "FAC_articles = './log/FAC_all_articles.txt'\n",
    "#get_articles(FA_archives, FA_articles)\n",
    "get_articles(FAC_archives, FAC_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the XML files\n",
    "The next step is to go to (wiki/Special:Export)[https://en.wikipedia.org/wiki/Special:Export] and use the content of FA(C)_archives to export the current state, including templates of alle Featured Articles. The two resulting files were named FA.xml (for the sucessfull) and FAC.xml (for the unsucessfull ones). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the nominated articles and time of nomination\n",
    "MyNominationHandler is in charge of \n",
    "1. finding all candidates\n",
    "2. find the point in time when they were nominated\n",
    "3. find the last comment (will be used in handling double nominations)\n",
    "\n",
    "It will produce an .csv file containing all nominations found in the .xml file and write these to ./data/ . A dictionary containing all problematic entries will be stored in ./tmp/ as a pickle object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0 is out of bounds for axis 0 with size 0\n",
      "Problematic entry in 5114\n",
      "seems like no title was found.\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "Problematic entry in 5291\n",
      "seems like no title was found.\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "Problematic entry in 6087\n",
      "seems like no title was found.\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "#Parse Sucessful Nominations\n",
    "nom_handler = MyNominationHandler('./data/FA_nomination.csv', './tmp/FA_prob.pkl')\n",
    "nom_handler.parse('./data/FA.xml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem parsing date in 613\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "# Parse Unsucsessful Nominations\n",
    "nom_handler = MyNominationHandler('./data/FAC_nomination.csv', './tmp/FAC_prob.pkl')\n",
    "nom_handler.parse('./data/FAC.xml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all all revisions in which an article was present\n",
    "To determine the end of the nomination period the last comment is an unreliable indicator. \n",
    "Thus we parse the whole revision history of the featured article discussion and store all articles currently nominated. \n",
    "Since we need more than the last 1000 revisions, we cannot use wiki/Special:Export. I used [WikiEvent](http://algo.uni-konstanz.de/software/wikievent/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2004-02-13 19:07:35 is already in the dict. \n",
      " This schould not have happend! (More than once)\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "end_handler = MyEndHandler('data/FAC_ends.pkl', 'tmp/FAC_ends_prob.pkl')\n",
    "end_handler.parse('./data/FAChistory.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and merge data\n",
    "#### Sucessfull nominations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiple nominations: 50\n",
      "number of nominations:                 5280 \n",
      "artilcles in ends:                10451\n",
      "unique articles in nominations:       5255 \n",
      "articles in merged data Frame:            5157\n",
      "5157 10451\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "FA_controll = process.merge('./data/FA_nomination.csv', './data/FAC_ends.pkl')\n",
    "FA_controll = process.decide_end(FA_controll)\n",
    "FA_controll.drop('dates', 1)\n",
    "FA_controll.to_csv('./res/FA_merged.csv', sep=';')\n",
    "#FA_controll = FA_controll[['title', 'start_date', 'date_nomination', 'end_date']]\n",
    "#FA_controll.to_csv('./data/FA_controll.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsucessfull nominations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiple nominations: 1845\n",
      "number of nominations:                 4966 \n",
      "artilcles in ends:                10451\n",
      "unique articles in nominations:       3885 \n",
      "articles in merged data Frame:            3774\n",
      "3774 10451\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "FAC_controll = process.merge('./data/FAC_nomination.csv', './data/FAC_ends.pkl')\n",
    "FAC_controll = process.decide_end(FAC_controll)\n",
    "FAC_controll.drop('dates', 1)\n",
    "FAC_controll.to_csv('./res/FAC_merged.csv', sep=';')\n",
    "FAC_controll = FAC_controll[['title', 'start_date', 'date_nomination', 'end_date']]\n",
    "FAC_controll.to_csv('./data/FAC_controll.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "with gzip.open('./data/enwiki_history.xml.gz') as input_file:\n",
    "    MyRevisionsHandler('./res/revisions_FAC.csv', './data/FAC_controll.csv', True).parse(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "with gzip.open('./data/enwiki_history.xml.gz') as input_file:\n",
    "    MyRevisionsHandler('./res/revisions_FA.csv', './data/FA_controll.csv', True).parse(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>title</th>\n",
       "      <th>date_nomination</th>\n",
       "      <th>date_last_comment</th>\n",
       "      <th>has_duplicate</th>\n",
       "      <th>dates</th>\n",
       "      <th>end_date</th>\n",
       "      <th>start_date</th>\n",
       "      <th>nomination_period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2174</th>\n",
       "      <td>3113</td>\n",
       "      <td>Elvis Presley</td>\n",
       "      <td>2008-02-11 22:36:00</td>\n",
       "      <td>2010-02-23 19:28:00</td>\n",
       "      <td>False</td>\n",
       "      <td>[2006-10-21 19:41:14, 2006-10-21 20:00:36, 200...</td>\n",
       "      <td>2010-02-23 19:28:00</td>\n",
       "      <td>2008-01-28 22:36:00</td>\n",
       "      <td>742 days 20:52:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx          title     date_nomination   date_last_comment  \\\n",
       "2174  3113  Elvis Presley 2008-02-11 22:36:00 2010-02-23 19:28:00   \n",
       "\n",
       "      has_duplicate                                              dates  \\\n",
       "2174          False  [2006-10-21 19:41:14, 2006-10-21 20:00:36, 200...   \n",
       "\n",
       "                end_date          start_date nomination_period  \n",
       "2174 2010-02-23 19:28:00 2008-01-28 22:36:00 742 days 20:52:00  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FA_controll.loc[FA_controll.title == 'Elvis Presley',]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Wiki Project",
   "language": "python",
   "name": "wiki_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
